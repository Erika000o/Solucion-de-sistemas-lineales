{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8ST8v1P9eDFXLOv/ZUzdJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erika000o/Solucion-de-sistemas-lineales/blob/main/Soluci%C3%B3n_Num%C3%A9rica_de_Sistemas_de_Ecuaciones_No_Lineales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Método de Newton-Raphson (multivariable)"
      ],
      "metadata": {
        "id": "7m_GjjsZqf58"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjjugJGzopk2",
        "outputId": "bd86b215-013d-4b8b-8607-26da268025f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning:El método no converge después de 90iteraciones\n",
            "Pesos y bias óptimos: [-3.72558198  2.68778388  3.0377981 ]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Definimos la función de error como una función no lineal de los pesos w1, w2 y b\n",
        "def error(X):\n",
        "    w1, w2, b = X\n",
        "    return np.array([\n",
        "        np.exp(w1) + w2 + b - 1,  # Primera ecuación que involucra los pesos\n",
        "        w1**2 + w2**2 - 4,  # Segunda ecuación (penaliza valores altos de los pesos)\n",
        "        w1 + w2 + b - 2  # Tercera ecuación (regularización simple)\n",
        "    ])\n",
        "\n",
        "# Jacobiano de la función de error (derivadas parciales con respecto a los pesos)\n",
        "def jacobiano(X):\n",
        "    w1, w2, b = X\n",
        "    return np.array([\n",
        "        [np.exp(w1), 1, 1],  # Derivada de la primera ecuación respecto a w1, w2 y b\n",
        "        [2*w1, 2*w2, 0],  # Derivada de la segunda ecuación respecto a w1, w2 y b\n",
        "        [1, 1, 1]  # Derivada de la tercera ecuación respecto a w1, w2 y b\n",
        "    ])\n",
        "\n",
        "# Método de Newton-Raphson para encontrar los pesos óptimos\n",
        "def newton_raphson(error, J, X0, tol=1e-6, max_iter=90):\n",
        "    X = X0\n",
        "    for i in range(max_iter):\n",
        "        J_inv = np.linalg.inv(J(X))  # Invertir el jacobiano en el punto actual\n",
        "        F_val = error(X)  # Evaluar la función de error en el punto actual\n",
        "        X_new = X - np.dot(J_inv, F_val)  # Actualizar los pesos\n",
        "        if np.linalg.norm(X_new - X, ord=np.inf) < tol:  # Verificar convergencia\n",
        "            return X_new\n",
        "        X = X_new  # Actualizar los pesos actuales\n",
        "    print(\"Warning:El método no converge después de {}iteraciones\".format(max_iter))\n",
        "    return X\n",
        "\n",
        "# Estimación inicial de los pesos y bias\n",
        "X0 = np.array([0.5, 0.5, 0.5])\n",
        "# Llamada al método de Newton-Raphson para minimizar el error\n",
        "solucion = newton_raphson(error, jacobiano, X0)\n",
        "print(\"Pesos y bias óptimos:\", solucion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Método de Broyden (una variante de Newton sin derivadas)"
      ],
      "metadata": {
        "id": "0ZlE1O7ZqxVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import broyden1\n",
        "\n",
        "# Función de error no lineal\n",
        "def error(X):\n",
        "    w1, w2, b = X\n",
        "    return np.array([\n",
        "        w1**2 + w2**2 + b**2 - 5,  # Función de error sobre los pesos\n",
        "        np.tanh(w1) + w2 + b - 1,  # Función de activación no lineal\n",
        "        w1 + w2 + b - 2  # Regularización simple\n",
        "    ])\n",
        "\n",
        "# Método de Broyden sin calcular derivadas explícitamente\n",
        "X0 = np.array([1.0, 1.0, 1.0])  # Estimación inicial de los pesos\n",
        "solucion = broyden1(error, X0, f_tol=1e-6)  # Uso del método de Broyden\n",
        "print(\"Pesos óptimos:\", solucion)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRMxkB4kqyNo",
        "outputId": "b4ba1f76-1c93-412e-c660-8afd3dbb98ac"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pesos óptimos: [ 1.9611792  -0.7398733   0.77869377]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Método de Punto Fijo"
      ],
      "metadata": {
        "id": "u5dURS6iq-Bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Definimos la función de ajuste para el método de punto fijo\n",
        "def G(X):\n",
        "    w1, w2, b = X  # Desempaquetamos el vector X en los valores de los pesos w1, w2, y b.\n",
        "\n",
        "    # Actualización de w1 con una raíz cuadrada\n",
        "    # Aseguramos que el argumento de la raíz cuadrada (4 - w2^2) sea no negativo.\n",
        "    # Si w2^2 > 4, se asigna un valor seguro (0) para evitar errores de valor complejo.\n",
        "    w1_new = np.sqrt(np.abs(4 - w2*2)) if w2*2 <= 4 else 0\n",
        "\n",
        "    # Actualización de w2 basada en una función exponencial\n",
        "    # Limitamos w1 para evitar que exp(w1) sea demasiado grande. Si w1 excede el logaritmo de 2, asignamos 0.\n",
        "    # Esto previene que exp(w1) cause valores inestables o crecimientos excesivos.\n",
        "    w2_new = 1 - np.exp(w1) if w1 < np.log(2) else 0\n",
        "\n",
        "    # Actualización de b como la suma de w1 y w2 menos 2 (sin restricciones adicionales).\n",
        "    b_new = w1 + w2 - 2\n",
        "\n",
        "    # Retornamos el nuevo vector de pesos y bias\n",
        "    return np.array([w1_new, w2_new, b_new])\n",
        "\n",
        "# Método de Punto Fijo para ajustar los pesos\n",
        "def punto_fijo(G, X0, tol=1e-6, max_iter=100):\n",
        "    X = X0  # Inicializamos X con el valor inicial proporcionado.\n",
        "\n",
        "    # Iteramos hasta el número máximo de iteraciones (max_iter)\n",
        "    for i in range(max_iter):\n",
        "        X_new = G(X)  # Aplicamos la función G para actualizar los pesos y el bias.\n",
        "\n",
        "        # Calculamos la diferencia entre la nueva estimación (X_new) y la anterior (X).\n",
        "        # Si la norma infinita (la mayor diferencia en los componentes) es menor que la tolerancia (tol),\n",
        "        # consideramos que el algoritmo ha convergido y retornamos el resultado.\n",
        "        if np.linalg.norm(X_new - X, ord=np.inf) < tol:\n",
        "            return X_new\n",
        "\n",
        "        # Si no ha convergido, actualizamos X con los nuevos valores de X_new.\n",
        "        X = X_new\n",
        "\n",
        "    # Si después de todas las iteraciones el método no ha convergido, se lanza un error con un mensaje.\n",
        "    raise ValueError(\"El método no converge después de {} iteraciones\".format(max_iter))\n",
        "\n",
        "# Estimación inicial de los pesos\n",
        "X0 = np.array([1.0, 1.0, 1.0])  # Vector inicial para los pesos w1, w2 y bias b.\n",
        "\n",
        "# Llamada al método de punto fijo para ajustar los pesos\n",
        "solucion = punto_fijo(G, X0)  # Ejecutamos el algoritmo con la función G y la estimación inicial X0.\n",
        "print(\"Pesos óptimos:\", solucion)  # Mostramos los pesos y el bias óptimos encontrados."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2p17U3CEq-zc",
        "outputId": "6b154448-7e69-4076-9e55-6cac47de16aa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pesos óptimos: [2. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Método de la Secante Multivariable"
      ],
      "metadata": {
        "id": "htzj4KY9rCsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Definimos la función de error que depende de los pesos w1, w2 y el bias b\n",
        "def error(X):\n",
        "    w1, w2, b = X\n",
        "    return np.array([\n",
        "        w1**2 + w2**2 + b**2 - 5,  # Función de error (penaliza grandes valores de los pesos)\n",
        "        np.tanh(w1) + w2 + b - 1,  # Activación no lineal con tangente hiperbólica\n",
        "        w1 + w2 + b - 2  # Regularización simple\n",
        "    ])\n",
        "\n",
        "# Método de la Secante Multivariable\n",
        "def secante_multivariable(F, X0, X1, tol=1e-6, max_iter=100):\n",
        "    for i in range(max_iter):\n",
        "        F0, F1 = F(X0), F(X1)  # Evaluamos la función de error en X0 y X1\n",
        "        diff = X1 - X0  # Diferencia entre X1 y X0\n",
        "        delta_F = F1 - F0  # Diferencia entre F(X1) y F(X0)\n",
        "\n",
        "        if np.linalg.norm(delta_F, ord=np.inf) < tol:  # Si la diferencia es pequeña, hemos convergido\n",
        "            return X1\n",
        "\n",
        "        # Actualización de los parámetros\n",
        "        X2 = X1 - np.dot(diff, F1) / np.dot(delta_F, delta_F) * delta_F\n",
        "        X0, X1 = X1, X2  # Actualizamos las estimaciones para la siguiente iteración\n",
        "\n",
        "    raise ValueError(\"El método no converge después de {} iteraciones\".format(max_iter))\n",
        "\n",
        "# Estimación inicial de los pesos y bias\n",
        "X0 = np.array([1.0, 1.0, 1.0])  # Primer punto\n",
        "X1 = np.array([0.9, 0.9, 0.9])  # Segundo punto cercano al primero\n",
        "\n",
        "# Aplicamos el método de la secante multivariable\n",
        "solucion = secante_multivariable(error, X0, X1)\n",
        "print(\"Pesos y bias óptimos:\", solucion)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzcePxRzrDT9",
        "outputId": "cb72dfe6-d9a0-4318-c71d-d6ad792d56e5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pesos y bias óptimos: [1.28610341 1.04218462 1.09962675]\n"
          ]
        }
      ]
    }
  ]
}